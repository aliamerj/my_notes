# Threading
Threading is about managing multiple "threads" or units of execution in a single program or process. Think of a thread as a mini-program within the main program, where each thread can run independently. This allows a program to do multiple things at once, like downloading a file in the background while the main program continues to interact with the user.

In Zig, managing threads is relatively low-level compared to languages with extensive multithreading libraries. Here's a quick look at the foundational concepts:

  - Binaries - Dormant programs, ready to execute but currently inactive.
  - Processes - Active instances of binaries. Each process has memory, CPU time, and other resources.
  - Threads - Smallest units of execution in a process. Threads share memory within their parent process but can run independently.

## Why Use Threads?

Threads can be beneficial in several ways:
  - Parallelism: With multiple cores, threads can run at the same time on different cores, doing more in less time.
  - Responsiveness: Long tasks can be run on background threads so the main program remains responsive.
  - Blocking I/O: Threads allow work to continue even if some threads are waiting on file reads or network calls.

However, threading comes with challenges, particularly with data races and deadlocks. This means we need to carefully manage memory and execution order to avoid bugs that are difficult to detect and fix.

## Creating Threads in Zig

In Zig, you can create threads using std.Thread.spawn. Here’s a basic example that shows two threads running different tasks:
```zig
const std = @import("std");

fn doWork(thread_id: usize) void {
    std.debug.print("Thread {} is running\n", .{thread_id});
}

pub fn main() void {
    const num_threads = 2;
    var threads: [num_threads]std.Thread = undefined;

    for (threads) |*thread, i| {
        thread.* = try std.Thread.spawn(std.heap.page_allocator, doWork, i);
    }

    // Wait for all threads to finish
    for (threads) |thread| {
        try thread.join();
    }
    std.debug.print("All threads completed\n", .{});
}
```
Explanation:

  - doWork is the function that each thread runs. We print out which thread is currently running.
  - We create num_threads threads, each running doWork.
  - join() makes sure the main program waits for all threads to complete before moving on.

This example is parallel: each thread can run independently, and they share the same memory space, so variables can be accessed by any thread (which requires careful handling).

## Key Benefits of Multithreading

Let's quickly go over why threads can be so helpful:

  - Parallelism: Running tasks truly in parallel. For example, on a machine with 4 cores, 4 threads could theoretically run at the same time, handling CPU-heavy operations faster.
  - Improved Responsiveness: Keeping the main thread available for user interaction by offloading work to other threads.
  - Blocking I/O: Threads can allow work to proceed even if certain operations, like file access or network requests, are delayed.

## Challenges with Threading: Data Races and Deadlocks

A data race happens when two or more threads try to access the same data at the same time, and at least one of the accesses is a write. This leads to unpredictable results. To prevent this, we use mutexes or atomic operations to synchronize access to shared data.
In Zig, we can use std.Thread.Mutex to control access:
```zig
const std = @import("std");

const data = struct {
    counter: usize,
    lock: std.Thread.Mutex,
};

fn increment(shared: *data) void {
    shared.lock.lock();
    defer shared.lock.unlock();
    shared.counter += 1;
    std.debug.print("Counter incremented to {}\n", .{shared.counter});
}

pub fn main() void {
    var shared = data{
        .counter = 0,
        .lock = std.Thread.Mutex{},
    };

    var t1 = try std.Thread.spawn(std.heap.page_allocator, increment, &shared);
    var t2 = try std.Thread.spawn(std.heap.page_allocator, increment, &shared);

    try t1.join();
    try t2.join();

    std.debug.print("Final counter value: {}\n", .{shared.counter});
}
```
Explanation:

  - Mutex prevents other threads from accessing counter while one thread is modifying it.
  - lock() and unlock() ensure only one thread changes counter at any given time, preventing a data race.

## Alternatives to Multithreading

If threading becomes too complex, some alternatives include:

  - Non-blocking I/O: Performing operations asynchronously without needing multiple threads.
  - Event-driven systems: Handling many tasks using a single thread by switching between tasks as they become ready (often used in GUI and network servers).

## When to Avoid Threads

Threading can be powerful but isn't always the right solution. Using threads unnecessarily can lead to harder-to-maintain code, increase the chances of bugs, and lead to performance costs in some cases.

In summary:

  - Threads are useful for parallelism, responsiveness, and handling I/O.
  - They share memory within a process, which means synchronization is essential.
  - Zig provides basic tools to handle threads and synchronization, which require caution to prevent bugs like data races and deadlocks.

## Threading Patterns: Thread-per-Connection and Event-Driven
  1) Thread-per-Connection:
      In the thread-per-connection model, each connection (or unit of work) is handled by one dedicated thread. Imagine a restaurant where each customer gets a dedicated server to take care of them from start to finish. Here, the thread (server) is assigned a connection (customer) and stays with it until everything’s complete.
      - Example: A simple web server could create a new thread for each client request and handle the client’s data processing independently.
      - Pros: Straightforward to code, especially when handling blocking I/O. If one connection is slow, it only affects its assigned thread.
      - Cons: If there are thousands of connections, there will be thousands of threads, which can eat up memory and CPU resources fast.
      In Zig, we might use std.Thread.spawn to create threads. Here’s an example:
  ```zig
      const std = @import("std");

pub fn handle_connection(id: u32) void {
    std.debug.print("Handling connection {d}\n", .{id});
    // Simulate work
    std.time.sleep(1 * std.time.ns_per_s);
}

pub fn main() void {
    var threads: [10]std.Thread = undefined;

    // Create a thread for each connection
    for (threads) |*thread, i| {
        thread.* = std.Thread.spawn(handle_connection, .{ i }) catch |err| {
            std.debug.print("Failed to create thread: {s}\n", .{err});
            continue;
        };
    }

    // Wait for all threads to finish
    for (threads) |thread| {
        thread.wait() catch |err| {
            std.debug.print("Failed to join thread: {s}\n", .{err});
        };
    }
}
```
Each thread in this code would handle a single connection, outputting its progress and then sleeping to simulate work.

2) Event-Driven Threading:

The event-driven model avoids creating many threads by relying on a single (or few) threads with asynchronous I/O. Here, the application processes work by receiving events in an event loop, similar to how waitstaff in a food court handle multiple customers as each order is ready, rather than dedicating one server per customer.

  - Example: In a high-load server, the server can use one or a few threads and handle multiple requests asynchronously. Instead of waiting for each request’s I/O to complete, the thread moves to the next request.
  - Pros: Reduces the number of threads, saving memory and resources. Great for applications handling thousands of connections.
  - Cons: More complex code, requires carefully handling asynchronous events.

In Zig, async functions and await allow event-driven models. Here’s an example:
```zig
const std = @import("std");

pub fn async_handle_connection(id: u32) !void {
    std.debug.print("Handling connection {d} asynchronously\n", .{id});
    try std.time.sleep(1 * std.time.ns_per_s); // Simulate async work
}

pub fn main() !void {
    var connections = [_]u32{0, 1, 2, 3, 4};
    for (connections) |conn| {
        async_handle_connection(conn) catch |err| {
            std.debug.print("Failed to handle connection: {s}\n", .{err});
        };
    }
}
```
This code schedules several asynchronous tasks, and each async_handle_connection can run independently without blocking others.

### Concurrency vs. Parallelism
  - Concurrency: Multiple tasks (or threads) are running during overlapping time periods. It’s like cooking several dishes at once but working on each only when you have time. It doesn’t necessarily mean they’re running at the exact same time.
  - Parallelism: This means tasks are running at exactly the same time, utilizing multiple CPU cores. Each dish would be prepared by a different cook simultaneously.

Concurrency is achievable on single-core systems, while parallelism needs multi-core processors.

### Race Conditions
Race conditions occur when multiple threads access and modify shared data at the same time, leading to unpredictable results. For example, consider an ATM withdrawing money. If two people try to withdraw from the same account at the same time, both could check the balance before one completes the transaction, allowing them to withdraw more than the account holds.

To fix race conditions, we use synchronization methods, like locks or atomic operations. Here’s an example:
```zig
const std = @import("std");

const Account = struct {
    balance: i32,
    mutex: std.Thread.Mutex,
    
    pub fn withdraw(self: *Account, amount: i32) bool {
        self.mutex.lock(); // Lock the critical region
        defer self.mutex.unlock(); // Ensure unlock after work

        if (self.balance < amount) {
            return false;
        }
        self.balance -= amount;
        return true;
    }
};

pub fn main() !void {
    var account = Account{ .balance = 1000, .mutex = std.Thread.Mutex.init() };

    var thread = try std.Thread.spawn(Account.withdraw, .{ &account, 500 });
    try account.withdraw(500);

    thread.wait() catch {};
    std.debug.print("Final balance: {d}\n", .{account.balance});
}
```
In this example, using a Mutex prevents the balance from being accessed simultaneously, ensuring that two threads won’t withdraw money at the same time.
...
### Synchronization

Race Conditions and Critical Regions: Imagine you have a piece of code that accesses shared data (like a bank balance). If two threads (units of execution) attempt to access this shared data simultaneously, unexpected outcomes can occur. This happens because each thread might interfere with the other's operation—a situation we call a "race condition."

To avoid this, we protect critical regions (sections where shared data is accessed) by using synchronization mechanisms. When we do this, we ensure that only one thread accesses the data at a time.

**Atomic Operations:** An operation is atomic if it is indivisible, meaning it cannot be interrupted. Critical regions are generally not atomic, so without safeguards, one thread might change data in the middle of another thread’s operation, resulting in unpredictable results.

#### Mutexes

What is a Mutex? A mutex (short for "mutual exclusion") is like a lock. If you want only one thread to access a critical region at a time, you use a mutex. Think of it as a locked room that only one person can enter at a time.

  - Locking with a Mutex: Before a thread enters a critical region, it must "lock" the mutex, preventing other threads from entering.
  - Unlocking with a Mutex: After the thread completes its work, it "unlocks" the mutex, allowing another thread to enter.

Here’s a Zig example of using a mutex to avoid a race condition:
```zig
const std = @import("std");

var shared_balance: i32 = 1000;
const mutex = std.Thread.Mutex.init(); // Initialize the mutex

fn withdraw(amount: i32) i32 {
    // Lock the mutex before accessing the critical region
    mutex.lock();
    defer mutex.unlock(); // Ensure unlock happens even if there's an early return

    if (shared_balance < amount) {
        return -1; // Not enough balance
    }

    shared_balance -= amount;
    return 0; // Success
}

pub fn main() void {
    // Example of calling withdraw concurrently
}
```
In this example, the mutex.lock() prevents multiple threads from changing shared_balance at the same time. The defer mutex.unlock() ensures the mutex is unlocked after the critical region is complete.

#### Lock Data, Not Code
This principle encourages us to focus on protecting data instead of locking large blocks of code. By associating locks with specific pieces of data (like the shared_balance variable in our example), we make it clear which resources are being protected. This improves code clarity and reduces accidental misuse.

#### Deadlocks
What is a Deadlock? A deadlock occurs when two or more threads are waiting indefinitely for each other to release a mutex, creating a circular dependency. This often happens when threads lock multiple resources in a different order.

**Avoiding Deadlocks:** A simple rule to prevent deadlocks is to always acquire multiple mutexes in a consistent order.

Example of Deadlock-Free Code:
```zig
const mutexA = std.Thread.Mutex.init();
const mutexB = std.Thread.Mutex.init();

fn safe_function() void {
    mutexA.lock();
    defer mutexA.unlock();

    mutexB.lock();
    defer mutexB.unlock();

    // Perform work here safely
}
```
In this setup, if all threads acquire mutexA before mutexB, we can avoid deadlocks.

#### Real-World Example: Priority Inversion
Imagine the Mars Pathfinder situation mentioned earlier: a low-priority thread holds a mutex that a high-priority thread needs, but a medium-priority thread preempts the low-priority one, effectively blocking the high-priority thread. This is known as priority inversion.

To handle this, some systems implement priority inheritance, where a thread holding a resource temporarily "inherits" the priority of a higher-priority thread that is waiting for that resource.

### Practical Tips for Zig
  - Use Mutexes Appropriately: Only lock critical regions, not entire functions or extensive blocks.
  - Avoid Nested Locks: If possible, use a single mutex to prevent deadlocks.
  - Consistent Lock Ordering: If multiple locks are needed, always acquire them in the same order.

### Pthreads
Pthreads, or POSIX threads, is a standard for multithreading on Unix-like systems. Even though the Linux kernel doesn’t have built-in threads (only processes), it provides the basics to allow threading with functions like clone(). Pthreads standardize how threads work across different Unix systems, making it easier for developers to write multithreaded applications in C and C++ on these platforms.

#### Why Use Pthreads?

Multithreading is valuable for tasks that benefit from parallel execution. Examples include web servers handling multiple client requests, video processing, and complex calculations. While many large projects create their own threading libraries (like Android or Mozilla), Pthreads remains a robust, widely-used standard for C and C++.

#### Linux Threading Implementations: LinuxThreads and NPTL

  - LinuxThreads: The original implementation of Pthreads in Linux, which worked but wasn’t very efficient because it didn’t have much support from the kernel.
  - NPTL (Native POSIX Thread Library): The current standard for threading on Linux, introduced with Linux kernel 2.6. NPTL fixed LinuxThreads’ issues and improved scalability, allowing thousands of threads in one process without major slowdowns.

#### Basic Concepts and Functions in Pthreads

  - Creating Threads with pthread_create: This function creates a new thread, which starts executing a function you specify. Each thread runs independently but shares resources (memory, open files) with other threads in the same process.
  - Thread IDs (pthread_self and pthread_equal): Each thread has a unique ID within the process, like a process ID (PID) but for threads. You can get a thread’s ID with pthread_self and check if two threads have the same ID with pthread_equal.
  - Terminating Threads: A thread can end by:
      - Finishing its function (like returning from main()).
      - Explicitly calling pthread_exit.
      - Being canceled by another thread with pthread_cancel.

    - Managing Thread Exit: Threads can exit by simply returning from their function or calling pthread_exit with a result that can be picked up by any thread waiting for it.

#### Moving to Code: Implementing Pthreads in Zig
Since Zig doesn’t have a direct Pthreads library, we’ll use Zig’s FFI (Foreign Function Interface) to call Pthreads functions. Here’s how to implement the basics:

  1. Creating a Thread in Zig: We can call pthread_create by importing it, defining the function signature in Zig, and using it to start a thread.
```zig
const std = @import("std");
const pthread = @cImport({
    @cInclude("pthread.h");
});

pub fn main() void {
    var thread_id: pthread.pthread_t = undefined;
    const err = pthread.pthread_create(
        &thread_id,
        null,
        startRoutine,
        null,
    );
    if (err != 0) {
        std.debug.print("Error creating thread: {}\n", .{err});
        return;
    }
    std.debug.print("Thread created with ID: {}\n", .{thread_id});
    _ = pthread.pthread_join(thread_id, null);
}

fn startRoutine(arg: ?*std.c_void) ?*std.c_void {
    std.debug.print("Hello from thread!\n", .{});
    return null;
}
```
Explanation:

  - We create a new thread with pthread_create.
  - startRoutine is the function that the thread will run. It must return ?*std.c_void and take a ?*std.c_void argument, as per Pthread requirements.
  - pthread_join waits for the thread to finish before main exits

### Joining Threads
When you create threads, they run independently. Joining a thread means waiting for it to finish its work before proceeding. Think of it as ensuring one task finishes before starting another, which helps synchronize threads.
#### Why Join Threads?
- To ensure a thread completes its work before the program exits or another task depends on its result.
- Helps avoid unexpected program behavior, like missing data or partial results.
#### Behavior:
- If the thread has already finished, join returns immediately.
- If not, it blocks the caller until the thread terminates.
#### Zig Equivalent:
In Zig, you manage threads using the std.Thread module. The join operation ensures that the main program waits for threads to complete.

#### Step 1: Zig Code for Thread Joining
Here's how we create and join threads in Zig:
```zig
const std = @import("std");

fn startThread(ctx: anytype) !void {
    std.debug.print("Hello from thread: {s}\n", .{ctx});
}

pub fn main() !void {
    const allocator = std.heap.page_allocator;

    // Define contexts for threads
    const ctx1 = "Thread 1";
    const ctx2 = "Thread 2";

    // Create threads
    var thread1 = try std.Thread.spawn(.{}, startThread, ctx1);
    var thread2 = try std.Thread.spawn(.{}, startThread, ctx2);

    // Wait for threads to complete
    try thread1.join();
    try thread2.join();

    std.debug.print("Both threads finished execution\n", .{});
}
```
#### Explanation:
1. Thread Creation:
The std.Thread.spawn function creates a thread, passing a function (startThread) and its context (ctx).

2. Thread Join:
After creating the threads, try thread1.join() ensures the main thread waits for thread1 to complete. Similarly, for thread2.

3. Output:
- The threads print their respective messages.
- Once both threads finish, the main thread resumes and prints the final message.

### Detaching Threads in Zig
Detaching a thread means the thread runs independently, and you don't have to wait for it to complete or manually clean up its resources. This is useful for tasks where you don’t care about the result, like background tasks that log data or send notifications.

In Zig, threading can be handled using std.Thread. A detached thread is one that releases its system resources automatically once it finishes execution, without needing to join it.

#### Why Detach Threads?
- Resource Management: Threads consume system resources. By default, resources allocated for a thread are not freed until it is joined. Detached threads clean up their resources once done, avoiding resource leaks.
- No Need to Wait: If a task does not need synchronization with the main thread, detaching avoids unnecessary blocking or waiting.
- Simpler Code: Detached threads don’t require explicit join() calls, which simplifies the thread management in some scenarios.

##### Detaching Threads in Zig
Zig doesn’t directly provide a pthread_detach equivalent, but we can implement similar behavior. Here's how:

1. Creating a Detached Thread:
- You can spawn a thread and ensure it doesn’t need to be joined by letting it manage its own lifetime.
- Use std.Thread.spawn to start a thread.
```zig
const std = @import("std");

fn backgroundTask() void {
    // Simulate some work in the background
    for (std.time.milliTimestamp() % 5) |i| {
        std.debug.print("Task running in background: {d}\n", .{i});
        std.time.sleep(1 * std.time.ns_per_s / 2);
    }
    std.debug.print("Task completed.\n", .{});
}

pub fn main() !void {
    const detached_thread = try std.Thread.spawn(backgroundTask, .{});

    // Do not join; let the thread run on its own
    std.debug.print("Main thread continuing without waiting.\n", .{});
    
    // Simulate other work
    std.time.sleep(1 * std.time.ns_per_s);
    std.debug.print("Main thread done.\n", .{});
}
```
#### Explanation of the Example
1. Thread Spawning:
std.Thread.spawn creates a new thread that executes backgroundTask.
The thread runs independently of the main thread.

2. No join() Call:
The main thread does not wait for the background thread to finish. Instead, it continues its own tasks.

3. Thread Lifecycle:
The spawned thread finishes execution, and its resources are cleaned up automatically.

#### When to Use Detached Threads:
- For tasks where results are irrelevant (e.g., logging, telemetry collection).
- Background operations that must continue even if the main program moves on.
