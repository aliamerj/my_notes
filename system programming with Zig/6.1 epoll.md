# Understanding epoll in Detail
epoll is an advanced and scalable I/O event notification system in Linux, designed to handle a large number of file descriptors (including network sockets) efficiently. It is commonly used in high-performance network servers to avoid the inefficiency of repeatedly checking (polling) all sockets or file descriptors.

## Key Concepts of epoll:
- File Descriptor: In Unix-like systems, every I/O resource (file, socket, pipe, etc.) is associated with a file descriptor.
- Blocking I/O: In blocking mode, an operation (e.g., reading from a socket) halts execution until data is available. This is inefficient for servers that handle many simultaneous connections.
- Non-blocking I/O: Non-blocking mode allows execution to continue, even when no data is available. It returns immediately with an indication that the data is not ready yet.
- epoll: Designed for monitoring multiple file descriptors to see if any are ready for I/O operations. It doesn't suffer from the inefficiency of older polling mechanisms (like poll() or select()) which scan all descriptors sequentially.

## epoll has three main operations:
1. epoll_create: Creates an epoll instance.
2. epoll_ctl: Adds, modifies, or removes file descriptors in the epoll instance.
3. epoll_wait: Waits for events on the file descriptors, blocking the process until one or more file descriptors are ready.

## Workflow of epoll:
1. Create an epoll instance.
2. Add file descriptors (like sockets) to the instance using epoll_ctl(). Specify what events to listen for (e.g., data ready for reading, socket ready for writing).
3. Use epoll_wait() to block until one or more file descriptors are ready for the events specified.
4. Process the events (e.g., read from a socket, write to a socket).
5. Repeat this loop to continuously handle I/O events.

## Types of epoll:
- Level-triggered (LT): This is the default mode. epoll_wait() will continuously report the file descriptor as "ready" as long as the condition holds (e.g., data is available for reading).
- Edge-triggered (ET): In this mode, epoll_wait() only reports the readiness of the file descriptor when it transitions from not ready to ready. It’s more efficient but requires the application to fully process the event when notified.

## How to Use epoll in Zig
In Zig, epoll is not directly included in the standard library, but you can use system calls to interact with it. Zig gives you the flexibility to interact with low-level Linux system calls, including epoll.

### Here’s how to use epoll in Zig, step by step:
#### Example: A TCP Server using epoll

- Step 1: Creating and Initializing epoll
```zig
const std = @import("std");
const linux = @import("std.os.linux");

fn setup_epoll() !std.os.fd_t {
    // Create an epoll instance
    const epfd = try linux.epoll_create1(0);  // flags set to 0 (no special flags)
    return epfd;
}
```
`epoll_create1(0)` creates an epoll instance and returns a file descriptor for it.

- Step 2: Adding File Descriptors to epoll

You need to add socket file descriptors to the epoll instance, telling epoll what kind of events to watch for (e.g., read events).
```zig
fn add_fd_to_epoll(epfd: std.os.fd_t, fd: std.os.fd_t, events: u32) !void {
    var event = linux.epoll_event{
        .events = events,
        .data = fd, // Store the file descriptor in the event's data field
    };
    try linux.epoll_ctl(epfd, linux.EPOLL_CTL_ADD, fd, &event);
}
```
`epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &event)` adds the file descriptor fd to the epoll instance epfd, monitoring it for the events specified (e.g., read readiness, write readiness).

- Step 3: Waiting for Events (epoll_wait)

Once the file descriptors are added, the server will wait for events (e.g., data ready for reading) using epoll_wait.

```zig
fn wait_for_events(epfd: std.os.fd_t, max_events: usize, timeout: i32) ![]linux.epoll_event {
    var events = try std.heap.page_allocator.alloc(linux.epoll_event, max_events);
    const num_events = try linux.epoll_wait(epfd, events.ptr, max_events, timeout);
    return events[0..num_events];
}
```
`epoll_wait(epfd, events.ptr, max_events, timeout)` blocks until one or more file descriptors are ready for I/O. It returns the number of events that occurred.

- Step 4: Full Example of TCP Server with epoll

Here’s a basic TCP server that uses epoll to handle incoming connections and read data:
```zig
const std = @import("std");
const linux = @import("std.os.linux");

pub fn main() !void {
    const allocator = std.heap.page_allocator;

    // Create an epoll instance
    const epfd = try setup_epoll();
    defer linux.close(epfd);

    // Create a TCP server socket
    const server_fd = try create_server_socket();
    defer linux.close(server_fd);

    // Add the server socket to epoll, monitoring it for incoming connections
    try add_fd_to_epoll(epfd, server_fd, linux.EPOLLIN);

    // Event loop
    while (true) {
        const events = try wait_for_events(epfd, 10, -1);
        for (events) |event| {
            if (event.data == server_fd) {
                // Accept new connection
                const client_fd = try linux.accept(server_fd, null, null);
                defer linux.close(client_fd);
                // Add the new client socket to epoll, monitoring it for reads
                try add_fd_to_epoll(epfd, client_fd, linux.EPOLLIN);
            } else {
                // Handle data from a connected client
                var buffer: [1024]u8 = undefined;
                const bytes_read = try linux.read(event.data, &buffer, buffer.len);
                if (bytes_read == 0) {
                    // Connection closed
                    try linux.close(event.data);
                } else {
                    // Echo the received data back to the client
                    try linux.write(event.data, &buffer, bytes_read);
                }
            }
        }
    }
}

fn setup_epoll() !std.os.fd_t {
    const epfd = try linux.epoll_create1(0);
    return epfd;
}

fn create_server_socket() !std.os.fd_t {
    const addr = linux.sockaddr_in{
        .sin_family = linux.AF_INET,
        .sin_port = std.os.htons(8080),
        .sin_addr = linux.in_addr{ .s_addr = std.os.htonl(linux.INADDR_ANY) },
        .sin_zero = [_]u8{0} ** 8,
    };

    const sockfd = try linux.socket(linux.AF_INET, linux.SOCK_STREAM, 0);
    try linux.bind(sockfd, &addr, @sizeOf(addr));
    try linux.listen(sockfd, 10);
    return sockfd;
}

fn add_fd_to_epoll(epfd: std.os.fd_t, fd: std.os.fd_t, events: u32) !void {
    var event = linux.epoll_event{
        .events = events,
        .data = fd,
    };
    try linux.epoll_ctl(epfd, linux.EPOLL_CTL_ADD, fd, &event);
}

fn wait_for_events(epfd: std.os.fd_t, max_events: usize, timeout: i32) ![]linux.epoll_event {
    var events = try std.heap.page_allocator.alloc(linux.epoll_event, max_events);
    const num_events = try linux.epoll_wait(epfd, events.ptr, max_events, timeout);
    return events[0..num_events];
}
```
## Key Points:
  1. Non-blocking nature: The server does not block on individual I/O operations, instead, it waits for events using epoll_wait().
  2. Efficient handling of many connections: epoll allows efficient monitoring of thousands of connections.
  3. Event-driven: The server only acts when events occur (e.g., new connections, data arriving).


## Edge-Triggered vs. Level-Triggered Epoll
1. Level-Triggered (default): The event will be repeatedly triggered as long as the file descriptor is ready.
   - Good for scenarios where you need continuous notifications until an action is taken.
   - Easier to implement but can lead to redundant notifications.
2. Edge-Triggered (EPOLLET): The event is triggered only once when the file descriptor changes to ready.
   - More efficient but harder to manage. You need to handle read/write readiness in full (e.g., read until EAGAIN).
   - Often recommended for high-performance applications.
#### Example (Edge-Triggered in Zig):
```zig
const std = @import("std");
const posix = std.os.linux;

pub fn main() !void {
    const epfd = try posix.epoll_create1(posix.EPOLL_CLOEXEC);
    defer posix.close(epfd);

    var events: [10]posix.epoll_event = undefined;
    const sock_fd = try posix.socket(posix.AF_INET, posix.SOCK_STREAM | posix.SOCK_NONBLOCK, 0);

    // Set up edge-triggered mode for sock_fd
    events[0] = posix.epoll_event{
        .events = posix.EPOLLIN | posix.EPOLLET,
        .data.fd = sock_fd,
    };

    try posix.epoll_ctl(epfd, posix.EPOLL_CTL_ADD, sock_fd, &events[0]);

    // Handling events here (simplified)
    const nfds = try posix.epoll_wait(epfd, &events, 10, -1);
    for (events_count in 0..nfds) {
        const ev = events[events_count];
        if (ev.events & posix.EPOLLIN != 0) {
            // Non-blocking read...
        }
    }
}
```
In edge-triggered mode, make sure you consume all available data, as you won't get notified again until a new event occurs.
##  Non-blocking I/O
Ensure that your sockets or file descriptors are non-blocking (SOCK_NONBLOCK). This is crucial because epoll works efficiently with non-blocking I/O, allowing the program to handle many connections without being blocked by individual requests.

## Handling Multiple File Descriptors
When using epoll, you can monitor multiple file descriptors (e.g., sockets) and handle events on them simultaneously. This is a common pattern in network servers where each client connection is a separate file descriptor.Avoid blocking operations in the event-handling loop, such as file I/O or heavy computation, to ensure other file descriptors get processed promptly.

## Managing Connection Timeouts
epoll_wait() has a timeout parameter, and you can set it to -1 for blocking indefinitely or specify a timeout value (in milliseconds).
Implementing a timeout is important when you need to close idle or stalled connections.

## Memory Considerations
You typically allocate memory for an array of epoll_event structures based on an estimated number of connections. However, using a dynamically growing array might be more scalable in high-concurrency environments.
If memory is tight, use event batching to handle more connections in fewer epoll_wait() calls.

## Event Coalescing
Epoll coalesces multiple events into one notification. For example, if data arrives before you process a previous event, epoll won’t trigger new notifications until you clear the readiness by reading all the data or processing all the pending events.

## Error Handling
Handle the EAGAIN or EWOULDBLOCK errors properly when reading/writing non-blocking file descriptors. These errors indicate that the operation would block and you should retry later when the file descriptor is ready again.
EPOLLERR and EPOLLHUP events indicate that a connection is in error or has been closed, so be sure to remove these descriptors from the epoll instance.

## Error Handling Example in Zig:
```zig
const std = @import("std");
const posix = std.os.linux;

pub fn main() !void {
    const epfd = try posix.epoll_create1(0);
    defer posix.close(epfd);

    var events: [10]posix.epoll_event = undefined;
    const nfds = try posix.epoll_wait(epfd, &events, 10, 1000);

    for (events_count in 0..nfds) {
        const ev = events[events_count];
        if (ev.events & posix.EPOLLERR != 0) {
            std.debug.warn("Socket error\n", .{});
            // Clean up the connection
        } else if (ev.events & posix.EPOLLHUP != 0) {
            std.debug.warn("Socket hang-up\n", .{});
            // Handle disconnection
        }
    }
}
```
## Performance Tuning
- Consider setting the SOMAXCONN and increasing the limit for the backlog queue in listen(). This helps in avoiding connection drops under heavy load.
- Adjust Linux kernel parameters like fs.file-max and net.core.somaxconn for better scalability.
- Utilize EPOLLRDHUP for better management of connection terminations.
